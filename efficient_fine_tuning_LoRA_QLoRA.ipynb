{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft bitsandbytes accelerate evaluate trl\n",
        "!pip install -q sentencepiece protobuf\n",
        "!pip install -q torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "9sTBAUaeTGGb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA Fine-Tuning"
      ],
      "metadata": {
        "id": "rAlbQasvSb03"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "r7arU-sGOh_t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType\n",
        ")\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "from typing import Dict, List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset_name: str = \"imdb\", max_samples: int = 5000):\n",
        "    \"\"\"\n",
        "    Prepare dataset for fine-tuning. Using IMDB for sentiment analysis as example.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Loading dataset: {dataset_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    if dataset_name == \"imdb\":\n",
        "        dataset = load_dataset(\"imdb\")\n",
        "        # Reduce dataset size for faster training\n",
        "        train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(max_samples))\n",
        "        test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
        "\n",
        "        # Format as instruction-following task\n",
        "        def format_instruction(example):\n",
        "            text = f\"Classify the sentiment of this movie review as positive or negative.\\n\\nReview: {example['text'][:500]}\\n\\nSentiment:\"\n",
        "            label = \" positive\" if example[\"label\"] == 1 else \" negative\"\n",
        "            return {\"text\": text + label}\n",
        "\n",
        "        train_dataset = train_dataset.map(format_instruction)\n",
        "        test_dataset = test_dataset.map(format_instruction)\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "wA5HXnW8OxBP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = prepare_dataset(\"imdb\", max_samples=3000)\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")\n",
        "print(f\"\\nExample training sample:\\n{train_data[0]['text'][:300]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUqcPuA_O4Wf",
        "outputId": "43f52c95-3751-497a-89af-c93a79cf0cca"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Loading dataset: imdb\n",
            "============================================================\n",
            "Training samples: 3000\n",
            "Test samples: 500\n",
            "\n",
            "Example training sample:\n",
            "Classify the sentiment of this movie review as positive or negative.\n",
            "\n",
            "Review: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"facebook/opt-350m\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the text data\"\"\"\n",
        "    outputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "A8gIfQODO9HD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = train_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_data.column_names\n",
        ")\n",
        "tokenized_test = test_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=test_data.column_names\n",
        ")"
      ],
      "metadata": {
        "id": "M8rnv8VJPGyG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model):\n",
        "    \"\"\"Calculate model size in MB\"\"\"\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    size_mb = (param_size + buffer_size) / 1024**2\n",
        "    return size_mb\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count total and trainable parameters\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params, trainable_params\n",
        "\n",
        "def print_model_stats(model, method_name: str):\n",
        "    \"\"\"Print comprehensive model statistics\"\"\"\n",
        "    total, trainable = count_parameters(model)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{method_name} - Model Statistics\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total parameters: {total:,}\")\n",
        "    print(f\"Trainable parameters: {trainable:,}\")\n",
        "    print(f\"Trainable %: {100 * trainable / total:.2f}%\")\n",
        "    print(f\"Model size: {get_model_size(model):.2f} MB\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "        print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n"
      ],
      "metadata": {
        "id": "IQJyIpS3PJIL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_TRAINING_ARGS = {\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"per_device_eval_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"logging_steps\": 50,\n",
        "    \"save_strategy\": \"no\",\n",
        "    \"eval_strategy\": \"steps\",\n",
        "    \"eval_steps\": 200,\n",
        "    \"fp16\": True,\n",
        "    \"report_to\": \"none\",\n",
        "}"
      ],
      "metadata": {
        "id": "90qqdbBRPU07"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Load base model\n",
        "base_model_lora = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "M6_DPey2PXgi"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank of the low-rank matrices\n",
        "    lora_alpha=32,  # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Which layers to apply LoRA to\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")"
      ],
      "metadata": {
        "id": "Ng1n41GiPgsd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model = get_peft_model(base_model_lora, lora_config)\n",
        "print_model_stats(lora_model, \"LoRA\")\n",
        "\n",
        "# Training arguments for LoRA\n",
        "lora_training_args = TrainingArguments(\n",
        "    **BASE_TRAINING_ARGS,\n",
        "    output_dir=\"./results_lora\",\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "lora_trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=lora_training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4CFctb8P6ws",
        "outputId": "ff18ff02-9d24-4271-b0e1-ad27bf710582"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "LoRA - Model Statistics\n",
            "============================================================\n",
            "Total parameters: 332,769,280\n",
            "Trainable parameters: 1,572,864\n",
            "Trainable %: 0.47%\n",
            "Model size: 637.71 MB\n",
            "GPU memory allocated: 0.98 GB\n",
            "GPU memory cached: 1.67 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and measure time\n",
        "print(\"Starting LoRA training...\")\n",
        "lora_start_time = time.time()\n",
        "lora_results = lora_trainer.train()\n",
        "lora_training_time = time.time() - lora_start_time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "7BhEU-bPQBMn",
        "outputId": "d431440e-0bd9-49ce-efc8-d5ce8f269c9a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting LoRA training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [188/188 03:55, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"LoRA Training completed in {lora_training_time:.2f} seconds\")\n",
        "print(f\"Final training loss: {lora_results.training_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoi0-7N-Q6EK",
        "outputId": "d95e9057-2b26-45e3-bc8e-b637505c8aaa"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA Training completed in 238.64 seconds\n",
            "Final training loss: 3.0122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_eval_results = lora_trainer.evaluate()\n",
        "print(f\"LoRA Evaluation loss: {lora_eval_results['eval_loss']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "xekDQqFxRwi0",
        "outputId": "ebb1ac96-c15c-48f1-db3f-0f5debb889e6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:14]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA Evaluation loss: 2.8743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QLoRA Fine-Tuning"
      ],
      "metadata": {
        "id": "RYqYldmHSSxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Q7OgqnAxSJre"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "FLdeAFUlSlki"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_qlora = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "X5XJSavKSpeh"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_qlora = prepare_model_for_kbit_training(base_model_qlora)\n",
        "\n",
        "qlora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")"
      ],
      "metadata": {
        "id": "x-ZUyF1dTMdg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qlora_model = get_peft_model(base_model_qlora, qlora_config)\n",
        "print_model_stats(qlora_model, \"QLoRA\")\n",
        "\n",
        "qlora_training_args = TrainingArguments(\n",
        "    **BASE_TRAINING_ARGS,\n",
        "    output_dir=\"./results_qlora\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWx7ta6-VEEq",
        "outputId": "ec38c73a-ccf2-492f-b9af-87b2549db0c1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "QLoRA - Model Statistics\n",
            "============================================================\n",
            "Total parameters: 181,250,048\n",
            "Trainable parameters: 1,572,864\n",
            "Trainable %: 0.87%\n",
            "Model size: 257.91 MB\n",
            "GPU memory allocated: 1.27 GB\n",
            "GPU memory cached: 1.73 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qlora_trainer = Trainer(\n",
        "    model=qlora_model,\n",
        "    args=qlora_training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "5aleqX3nVOIN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting QLoRA training...\")\n",
        "qlora_start_time = time.time()\n",
        "qlora_results = qlora_trainer.train()\n",
        "qlora_training_time = time.time() - qlora_start_time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "RohYBzWrVSdI",
        "outputId": "3f7b0dd2-7348-45e6-a001-dfd9c5a438bc"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting QLoRA training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [188/188 05:54, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nQLoRA Training completed in {qlora_training_time:.2f} seconds\")\n",
        "print(f\"Final training loss: {qlora_results.training_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1AvK4eTVV74",
        "outputId": "d530bb63-7037-4f1e-d4e2-243a8eb73022"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "QLoRA Training completed in 357.27 seconds\n",
            "Final training loss: 3.0609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qlora_eval_results = qlora_trainer.evaluate()\n",
        "print(f\"QLoRA Evaluation loss: {qlora_eval_results['eval_loss']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "9-vawXXfWt7e",
        "outputId": "f09124ea-b0fd-43c4-a207-25f164220853"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:18]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QLoRA Evaluation loss: 2.9067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Comparison"
      ],
      "metadata": {
        "id": "j_lLS5PjXgoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_total, lora_trainable = count_parameters(lora_model)\n",
        "qlora_total, qlora_trainable = count_parameters(qlora_model)\n",
        "\n",
        "comparison_data = {\n",
        "    \"Method\": [\"LoRA\", \"QLoRA\"],\n",
        "    \"Total Params\": [f\"{lora_total:,}\", f\"{qlora_total:,}\"],\n",
        "    \"Trainable Params\": [f\"{lora_trainable:,}\", f\"{qlora_trainable:,}\"],\n",
        "    \"Trainable %\": [\n",
        "        f\"{100 * lora_trainable / lora_total:.2f}%\",\n",
        "        f\"{100 * qlora_trainable / qlora_total:.2f}%\"\n",
        "    ],\n",
        "    \"Training Time\": [\n",
        "        f\"{lora_training_time:.2f}s\",\n",
        "        f\"{qlora_training_time:.2f}s\"\n",
        "    ],\n",
        "    \"Final Train Loss\": [\n",
        "        f\"{lora_results.training_loss:.4f}\",\n",
        "        f\"{qlora_results.training_loss:.4f}\"\n",
        "    ],\n",
        "    \"Eval Loss\": [\n",
        "        f\"{lora_eval_results['eval_loss']:.4f}\",\n",
        "        f\"{qlora_eval_results['eval_loss']:.4f}\"\n",
        "    ],\n",
        "    \"Model Size\": [\n",
        "        f\"{get_model_size(lora_model):.2f} MB\",\n",
        "        f\"{get_model_size(qlora_model):.2f} MB\"\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "OdFt_xAWWxZZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in comparison_data:\n",
        "    print(f\"{key:20s} | {comparison_data[key][0]:30s} | {comparison_data[key][1]:30s}\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "# Key insights\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KEY INSIGHTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"1. Memory Efficiency: QLoRA uses ~{get_model_size(lora_model) / get_model_size(qlora_model):.2f}x less memory than LoRA\")\n",
        "print(f\"2. Training Speed: QLoRA is {lora_training_time / qlora_training_time:.2f}x {'faster' if qlora_training_time < lora_training_time else 'slower'} than LoRA\")\n",
        "print(f\"3. Parameter Efficiency: Both methods train only {100 * lora_trainable / lora_total:.2f}% of parameters\")\n",
        "print(f\"4. Performance: Evaluation loss difference: {abs(lora_eval_results['eval_loss'] - qlora_eval_results['eval_loss']):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb9VPu4TXqbc",
        "outputId": "3307476f-bad3-4d8e-8e63-5fe26fa0c546"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method               | LoRA                           | QLoRA                         \n",
            "Total Params         | 332,769,280                    | 181,250,048                   \n",
            "Trainable Params     | 1,572,864                      | 1,572,864                     \n",
            "Trainable %          | 0.47%                          | 0.87%                         \n",
            "Training Time        | 238.64s                        | 357.27s                       \n",
            "Final Train Loss     | 3.0122                         | 3.0609                        \n",
            "Eval Loss            | 2.8743                         | 2.9067                        \n",
            "Model Size           | 637.71 MB                      | 257.87 MB                     \n",
            "====================================================================================================\n",
            "\n",
            "============================================================\n",
            "KEY INSIGHTS\n",
            "============================================================\n",
            "1. Memory Efficiency: QLoRA uses ~2.47x less memory than LoRA\n",
            "2. Training Speed: QLoRA is 0.67x slower than LoRA\n",
            "3. Parameter Efficiency: Both methods train only 0.47% of parameters\n",
            "4. Performance: Evaluation loss difference: 0.0323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(model, prompt: str, max_new_tokens: int = 15):\n",
        "    \"\"\"Generate text from a prompt\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.1,  # Very low temperature for deterministic output\n",
        "            do_sample=False,  # Greedy decoding for most likely answer\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Extract only the first word/line (the actual sentiment prediction)\n",
        "    # Stop at newline or after first complete word\n",
        "    if '\\n' in generated_text:\n",
        "        generated_text = generated_text.split('\\n')[0].strip()\n",
        "\n",
        "    # Take only the first word if it's a simple sentiment\n",
        "    words = generated_text.split()\n",
        "    if len(words) > 0 and words[0].lower() in ['positive', 'negative']:\n",
        "        return words[0]\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "FGUX3HnOazhN"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    \"Classify the sentiment of this movie review as positive or negative.\\n\\nReview: This film was absolutely incredible! The acting was superb.\\n\\nSentiment:\",\n",
        "    \"Classify the sentiment of this movie review as positive or negative.\\n\\nReview: Terrible waste of time. I want my money back.\\n\\nSentiment:\",\n",
        "]\n",
        "\n",
        "print(\"LoRA Model Predictions:\")\n",
        "print(\"-\" * 60)\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    response = generate_response(lora_model, prompt)\n",
        "    print(f\"Example {i}:\")\n",
        "    print(f\"Prompt: {prompt[:100]}...\")\n",
        "    print(f\"Predicted Sentiment: {response}\")\n",
        "\n",
        "print(\"QLoRA Model Predictions:\")\n",
        "print(\"-\" * 60)\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    response = generate_response(qlora_model, prompt)\n",
        "    print(f\"Example {i}:\")\n",
        "    print(f\"Prompt: {prompt[:100]}...\")\n",
        "    print(f\"Predicted Sentiment: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKZH20vua-S9",
        "outputId": "fcf0370c-9914-4290-db9f-b38e84b9e608"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA Model Predictions:\n",
            "------------------------------------------------------------\n",
            "Example 1:\n",
            "Prompt: Classify the sentiment of this movie review as positive or negative.\n",
            "\n",
            "Review: This film was absolute...\n",
            "Predicted Sentiment: positive\n",
            "Example 2:\n",
            "Prompt: Classify the sentiment of this movie review as positive or negative.\n",
            "\n",
            "Review: Terrible waste of time...\n",
            "Predicted Sentiment: negative\n",
            "QLoRA Model Predictions:\n",
            "------------------------------------------------------------\n",
            "Example 1:\n",
            "Prompt: Classify the sentiment of this movie review as positive or negative.\n",
            "\n",
            "Review: This film was absolute...\n",
            "Predicted Sentiment: positive\n",
            "Example 2:\n",
            "Prompt: Classify the sentiment of this movie review as positive or negative.\n",
            "\n",
            "Review: Terrible waste of time...\n",
            "Predicted Sentiment: negative\n"
          ]
        }
      ]
    }
  ]
}